"""
Wiki ì•± ë·° - ë©”ì¸ ê²€ìƒ‰ ë° ì •ë³´ ì•ˆë‚´
"""

from datetime import timezone
from django.shortcuts import get_object_or_404
# from django.db import transaction
# from django.utils import timezone
from django.db.models import Avg

import requests
from rest_framework import viewsets, status
from rest_framework.decorators import action
from rest_framework.response import Response
from drf_spectacular.utils import extend_schema, OpenApiParameter

# from places.models import Place
from .models import WikiPlace, WikiSearchHistory, Review, Report
from .serializers import (
    WikiSearchQuerySerializer,
    WikiPlaceSearchResultSerializer, 
    WikiPlaceDetailSerializer,
    WikiReviewSerializer,
    PopularKeywordSerializer
)
from typing import List, Dict, Optional, Tuple
# from .services import (
#     # search_places_by_keyword,
#     # parse_kakao_place_data,
#     get_popular_search_keywords
# )

from .service import google, openai

import logging

logger = logging.getLogger(__name__)


class WikiViewSet(viewsets.GenericViewSet):
    """ìœ„í‚¤ ë©”ì¸ ë·°ì…‹ - ì¥ì†Œ ê²€ìƒ‰, ìƒì„¸ ì •ë³´, ì¸ê¸° ê²€ìƒ‰ì–´"""
    queryset = WikiPlace.objects.all()

    @extend_schema(
        tags=["ğŸ”¥ìœ„í‚¤í˜ì´ì§€"],
        parameters=[WikiSearchQuerySerializer],
        responses={200: WikiPlaceSearchResultSerializer(many=True)},
        summary="3.1 ìœ„í‚¤ ê²€ìƒ‰ - ì¥ì†Œ ë° ì§€ì—­ ê²€ìƒ‰ ê°€ëŠ¥ â†’ í•«í•œ ì¥ì†Œ, ì§€ì—­ ì•ˆë‚´"
    )
    @action(detail=False, methods=["GET"])
    def search(self, request):
        """ìœ„í‚¤ ê²€ìƒ‰ ê¸°ëŠ¥ - ì¹´ì¹´ì˜¤ -> êµ¬ê¸€ API í™œìš©"""
        # ìš”ì²­ íŒŒë¼ë¯¸í„° ìœ íš¨ì„± ê²€ì‚¬
        query_serializer = WikiSearchQuerySerializer(data=request.query_params)
        query_serializer.is_valid(raise_exception=True)
        
        search_data = query_serializer.validated_data
        
        try:
            # êµ¬ê¸€ API í˜¸ì¶œ
            google_places = google.search_place(**search_data)
                        
            # # ê²€ìƒ‰ ê¸°ë¡ ì €ì¥ (ì„¸ì…˜ í‚¤ê°€ ìˆëŠ” ê²½ìš°)
            # if session_key:
            #     try:
            #         search_type = 'mixed'
            #         if place_name and not location_name:
            #             search_type = 'place_name'
            #         elif location_name and not place_name:
            #             search_type = 'location_name'
                    
            #         WikiSearchHistory.objects.create(
            #             search_query=search_query,
            #             search_type=search_type,
            #             result_count=len(search_results),
            #             session_key=session_key,
            #             search_longitude=user_longitude,
            #             search_latitude=user_latitude
            #         )
            #     except Exception as e:
            #         logger.warning(f"ê²€ìƒ‰ ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
        

            
        except Exception as e:
            logger.error(f"ìœ„í‚¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return Response(
                {'detail': f'ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
        return Response({"google_place" : google_places}, status=200)

    @extend_schema(
        tags=["ğŸ”¥ìœ„í‚¤í˜ì´ì§€"],
        parameters=[
            OpenApiParameter(name="place_id", description="ì¥ì†ŒID", required=True, type=str)
        ],
        responses={200: WikiPlaceDetailSerializer},
        summary="3.2.1 ê²°ê³¼ í™”ë©´ - AI ìš”ì•½ + ê¸°ë³¸ ì •ë³´ + í›„ê¸°"
    )
    @action(detail=False, methods=["GET"], url_path='detail')
    def place_detail(self, request):
        """ì¥ì†Œ ìƒì„¸ ì •ë³´ ì œê³µ - OpenAI API í™œìš© AI ìš”ì•½"""
        # ìš”ì²­ íŒŒë¼ë¯¸í„° ê²€ì¦
        place_id = request.query_params.get('place_id')
        
        if not place_id:
            return Response(
                {'detail': 'place_idëŠ” í•„ìˆ˜ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.'},
                status=status.HTTP_400_BAD_REQUEST
            )
        
        ai_summary = None
        try:
            search_details = google.search_detail(place_id=place_id) 
            shop_name =search_details.get("place_name")  
        
            # IDì— í•´ë‹¹í•˜ëŠ” ìœ„í‚¤ ëª¨ë¸ì˜ ë³„ì ì— ì ‘ê·¼
            # WikiPlace ì¡°íšŒ
            wiki_place = None
            
            
            wiki_place, created = WikiPlace.objects.get_or_create(
                google_place_id = place_id,
                defaults={
                    'shop_name': shop_name
                }
            )

            if not created: #ë“±ë¡ì€ ëëŠ”ë° ì´ë¦„ì´ ë¹„ì–´ìˆë‹¤ë©´! ë¦¬ë·° ë¨¼ì € ì“´ ê²½ìš°.. ë””ë²„ê¹…ìš©.
                if shop_name and not (wiki_place.shop_name and wiki_place.shop_name.strip()):
                    wiki_place.shop_name = shop_name
                    wiki_place.save(update_fields=["shop_name"])
        

            # í‰ì  ì •ë³´ ê³„ì‚°
            reviews = Review.objects.filter(wiki_place=wiki_place)
            review_score = 0.00
            if wiki_place:
                review_score = wiki_place.average_review_score
            else:
                # ì‹¤ì‹œê°„ í‰ì  ê³„ì‚°
                if reviews.exists():
                    avg_score = reviews.aggregate(avg=Avg('review_score'))['avg']
                    review_score = float(avg_score) if avg_score else 0.00
                else:
                    review_score = search_details.get("rating") #ì—†ë‹¤ë©´ êµ¬ê¸€ í‰ì 

            # ê²Œì‹œíŒ ë¦¬ë·° ì¡°íšŒ (ìµœì‹ ìˆœ/ì¶”ì²œìˆœ)
            reviews_content = wiki_place.reviews.order_by('-created_at')
            reviews_count = wiki_place.reviews.count()
            reviews_data = WikiReviewSerializer(
                reviews_content, many=True, context={'request': request}
            ).data #ì§ë ¬í™”

            ############################################################################################
            # ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘
            review_texts = [reviews.review_content for reviews in reviews_content if reviews.review_content]
            input_text = "\n\n".join(review_texts) #ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ í•©ì¹¨

            if review_texts:
                try:
                    ai_summary = openai.openai_summary(input_text = input_text)
                
                except requests.HTTPError as e:
                    status_code = e.response.status_code if e.response else "NoStatus"
                    detail = e.response.text if e.response else str(e)
                    return Response(
                        {"detail": f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {status_code} - {detail}"},
                        status=502
                    )
                except requests.RequestException as e:
                    return Response({"detail": f"openAI API í˜¸ì¶œ ì‹¤íŒ¨(ë„¤íŠ¸ì›Œí¬): {e}"}, status=502)

        except Exception as e:
            logger.error(f"ìœ„í‚¤ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return Response(
                {'detail': f'ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
        
        return Response(
            {
                "search_detail":search_details, # êµ¬ê¸€ api ì¡°íšŒ
                "average_review_score":review_score, # ìœ„í‚¤ë³„ì 
                "ai_summary": ai_summary, # AIìš”ì•½
                'reviews_count':reviews_count,
                "reviews_content":reviews_data
            }, 
            status=200

        # AI ìš”ì•½ ìƒì„± (ì•„ì§ ì—†ê±°ë‚˜ ì˜¤ë˜ëœ ê²½ìš°)
        # should_generate_ai_summary = (
        #     not wiki_place.ai_summation or
        #     not wiki_place.ai_summary_updated_at or
        #     (timezone.now() - wiki_place.ai_summary_updated_at).days > 30
        # )
    
        
        )
    
    def get_popular_search_keywords(limit: int = 10) -> List[Dict]:
        """ì¸ê¸° ê²€ìƒ‰ í‚¤ì›Œë“œ ì¡°íšŒ
        
        Args:
            limit: ë°˜í™˜í•  í‚¤ì›Œë“œ ê°œìˆ˜
        
        Returns:
            ì¸ê¸° ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸ [{"keyword": "í‚¤ì›Œë“œ", "count": íšŸìˆ˜}, ...]
        """
        from django.db.models import Count
        from .models import WikiSearchHistory
        
        # ìµœê·¼ 7ì¼ê°„ì˜ ê²€ìƒ‰ ê¸°ë¡ì—ì„œ ì¸ê¸° í‚¤ì›Œë“œ ì¶”ì¶œ
        from datetime import timedelta
        recent_date = timezone.now() - timedelta(days=7)
        
        popular_keywords = (
            WikiSearchHistory.objects
            .filter(created_at__gte=recent_date)
            .values('search_query')
            .annotate(search_count=Count('search_query'))
            .order_by('-search_count')[:limit]
        )
        
        return [
            {
                "keyword": item['search_query'],
                "count": item['search_count']
            }
            for item in popular_keywords
        ]

    @extend_schema(
        tags=["ìœ„í‚¤ ê¸°íƒ€"],
        parameters=[
            OpenApiParameter(name="limit", description="ë°˜í™˜í•  í‚¤ì›Œë“œ ê°œìˆ˜", required=False, type=int),
        ],
        responses={200: PopularKeywordSerializer(many=True)},
        description="ì¸ê¸° ê²€ìƒ‰ì–´ ëª©ë¡ ì¡°íšŒ"
    )
    @action(detail=False, methods=["GET"])
    def popular_keywords(self, request):
        """ì¸ê¸° ê²€ìƒ‰ì–´ ëª©ë¡ ë°˜í™˜"""
        limit = int(request.query_params.get('limit', 10))
        limit = min(max(limit, 1), 50)  # 1~50 ë²”ìœ„ë¡œ ì œí•œ
        
        try:
            keywords = get_popular_search_keywords(limit=limit)
            serializer = PopularKeywordSerializer(keywords, many=True)
            return Response(serializer.data, status=status.HTTP_200_OK)
        except Exception as e:
            logger.error(f"ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return Response(
                {'detail': 'ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )