"""
Wiki ì•± ë·° - ë©”ì¸ ê²€ìƒ‰ ë° ì •ë³´ ì•ˆë‚´
"""

from datetime import timezone
from django.shortcuts import get_object_or_404
# from django.db import transaction
# from django.utils import timezone
from django.db.models import Avg

import requests
from rest_framework import viewsets, status
from rest_framework.decorators import action
from rest_framework.response import Response
from drf_spectacular.utils import extend_schema, OpenApiParameter

# from places.models import Place
from .models import WikiPlace, WikiSearchHistory, Review, Report
from .serializers import (
    WikiSearchQuerySerializer,
    WikiPlaceSearchResultSerializer, 
    WikiPlaceDetailSerializer,
    WikiReviewSerializer,
    # WikiReviewCreateSerializer,
    # WikiReportSerializer,
    # WikiReportCreateSerializer,
    PopularKeywordSerializer
)
from .services import (
    search_places_by_keyword,
    parse_kakao_place_data,
    get_popular_search_keywords
)

from .service import google, openai

import logging

logger = logging.getLogger(__name__)


class WikiViewSet(viewsets.GenericViewSet):
    """ìœ„í‚¤ ë©”ì¸ ë·°ì…‹ - ì¥ì†Œ ê²€ìƒ‰, ìƒì„¸ ì •ë³´, ì¸ê¸° ê²€ìƒ‰ì–´"""
    queryset = WikiPlace.objects.all()

    @extend_schema(
        tags=["ğŸ”¥ìœ„í‚¤í˜ì´ì§€"],
        parameters=[WikiSearchQuerySerializer],
        responses={200: WikiPlaceSearchResultSerializer(many=True)},
        summary="3.1 ìœ„í‚¤ ê²€ìƒ‰ - ì¥ì†Œ ë° ì§€ì—­ ê²€ìƒ‰ ê°€ëŠ¥ â†’ í•«í•œ ì¥ì†Œ, ì§€ì—­ ì•ˆë‚´"
    )
    @action(detail=False, methods=["GET"])
    def search(self, request):
        """ìœ„í‚¤ ê²€ìƒ‰ ê¸°ëŠ¥ - ì¹´ì¹´ì˜¤ -> êµ¬ê¸€ API í™œìš©"""
        # ìš”ì²­ íŒŒë¼ë¯¸í„° ìœ íš¨ì„± ê²€ì‚¬
        query_serializer = WikiSearchQuerySerializer(data=request.query_params)
        query_serializer.is_valid(raise_exception=True)
        
        search_data = query_serializer.validated_data
        
        try:
            # êµ¬ê¸€ API í˜¸ì¶œ
            google_places = google.search_place(**search_data)
            
                
            #     # ê¸°ì¡´ Place ëª¨ë¸ì—ì„œ í•´ë‹¹ ì¥ì†Œ ì°¾ê¸° (ì¢Œí‘œ ê¸°ë°˜)
            #     existing_place = None
            #     try:
            #         # ì¢Œí‘œê°€ ë¹„ìŠ·í•œ ê¸°ì¡´ ì¥ì†Œ ì°¾ê¸° (ì˜¤ì°¨ í—ˆìš© ë²”ìœ„: 0.001ë„ ì•½ 100m)
            #         existing_place = Place.objects.filter(
            #             longitude__range=(place_data['longitude'] - 0.001, place_data['longitude'] + 0.001),
            #             latitude__range=(place_data['latitude'] - 0.001, place_data['latitude'] + 0.001)
            #         ).first()
            #     except Exception as e:
            #         logger.warning(f"ê¸°ì¡´ ì¥ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                
            
            # # ê²€ìƒ‰ ê¸°ë¡ ì €ì¥ (ì„¸ì…˜ í‚¤ê°€ ìˆëŠ” ê²½ìš°)
            # if session_key:
            #     try:
            #         search_type = 'mixed'
            #         if place_name and not location_name:
            #             search_type = 'place_name'
            #         elif location_name and not place_name:
            #             search_type = 'location_name'
                    
            #         WikiSearchHistory.objects.create(
            #             search_query=search_query,
            #             search_type=search_type,
            #             result_count=len(search_results),
            #             session_key=session_key,
            #             search_longitude=user_longitude,
            #             search_latitude=user_latitude
            #         )
            #     except Exception as e:
            #         logger.warning(f"ê²€ìƒ‰ ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
        

            
        except Exception as e:
            logger.error(f"ìœ„í‚¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return Response(
                {'detail': f'ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
        return Response({"google_place" : google_places}, status=200)

    @extend_schema(
        tags=["ğŸ”¥ìœ„í‚¤í˜ì´ì§€"],
        parameters=[
            OpenApiParameter(name="place_id", description="ì¥ì†ŒID", required=True, type=str)
        ],
        responses={200: WikiPlaceDetailSerializer},
        summary="3.2.1 ê²°ê³¼ í™”ë©´ - AI ìš”ì•½ + ê¸°ë³¸ ì •ë³´ + í›„ê¸°"
    )
    @action(detail=False, methods=["GET"], url_path='detail')
    def place_detail(self, request):
        """ì¥ì†Œ ìƒì„¸ ì •ë³´ ì œê³µ - OpenAI API í™œìš© AI ìš”ì•½"""
        # ìš”ì²­ íŒŒë¼ë¯¸í„° ê²€ì¦
        place_id = request.query_params.get('place_id')
        
        if not place_id:
            return Response(
                {'detail': 'place_idëŠ” í•„ìˆ˜ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.'},
                status=status.HTTP_400_BAD_REQUEST
            )
        
        ai_summary = None
        try:
            search_details = google.search_detail(place_id=place_id) 
            shop_name =search_details.get("place_name")  
        
            # IDì— í•´ë‹¹í•˜ëŠ” ìœ„í‚¤ ëª¨ë¸ì˜ ë³„ì ì— ì ‘ê·¼
            # WikiPlace ì¡°íšŒ
            wiki_place = None
            
            
            wiki_place, created = WikiPlace.objects.get_or_create(
                google_place_id = place_id,
                defaults={
                    'shop_name': shop_name
                }
            )

            if not created: #ë“±ë¡ì€ ëëŠ”ë° ì´ë¦„ì´ ë¹„ì–´ìˆë‹¤ë©´! ë¦¬ë·° ë¨¼ì € ì“´ ê²½ìš°.. ë””ë²„ê¹…ìš©.
                if shop_name and not (wiki_place.shop_name and wiki_place.shop_name.strip()):
                    wiki_place.shop_name = shop_name
                    wiki_place.save(update_fields=["shop_name"])
        

            # í‰ì  ì •ë³´ ê³„ì‚°
            reviews = Review.objects.filter(wiki_place=wiki_place)
            review_score = 0.00
            if wiki_place:
                review_score = wiki_place.average_review_score
            else:
                # ì‹¤ì‹œê°„ í‰ì  ê³„ì‚°
                if reviews.exists():
                    avg_score = reviews.aggregate(avg=Avg('review_score'))['avg']
                    review_score = float(avg_score) if avg_score else 0.00
                else:
                    review_score = search_details.get("rating") #ì—†ë‹¤ë©´ êµ¬ê¸€ í‰ì 

            # ê²Œì‹œíŒ ë¦¬ë·° ì¡°íšŒ (ìµœì‹ ìˆœ/ì¶”ì²œìˆœ)
            reviews_content = wiki_place.reviews.order_by('-created_at')
            reviews_count = wiki_place.reviews.count()
            reviews_data = WikiReviewSerializer(
                reviews_content, many=True, context={'request': request}
            ).data #ì§ë ¬í™”

            ############################################################################################
            # ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘
            review_texts = [reviews.review_content for reviews in reviews_content if reviews.review_content]
            input_text = "\n\n".join(review_texts) #ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ í•©ì¹¨

            if review_texts:
                try:
                    ai_summary = openai.openai_summary(input_text = input_text)
                
                except requests.HTTPError as e:
                    status_code = e.response.status_code if e.response else "NoStatus"
                    detail = e.response.text if e.response else str(e)
                    return Response(
                        {"detail": f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {status_code} - {detail}"},
                        status=502
                    )
                except requests.RequestException as e:
                    return Response({"detail": f"openAI API í˜¸ì¶œ ì‹¤íŒ¨(ë„¤íŠ¸ì›Œí¬): {e}"}, status=502)

        except Exception as e:
            logger.error(f"ìœ„í‚¤ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return Response(
                {'detail': f'ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
        
        return Response(
            {
                "search_detail":search_details, # êµ¬ê¸€ api ì¡°íšŒ
                "average_review_score":review_score, # ìœ„í‚¤ë³„ì 
                "ai_summary": ai_summary, # AIìš”ì•½
                'reviews_count':reviews_count,
                "reviews_content":reviews_data
            }, 
            status=200
            

        # IDì— í•´ë‹¹í•˜ëŠ” ìœ„í‚¤ ëª¨ë¸ì˜ ëŒ“ê¸€ì— ì ‘ê·¼ => AI ìš”ì•½ API í˜¸ì¶œ
        # reviews = Review.objects.filter(wiki_place=wiki_place).order_by('-created_at')[:10]
        
        
        # AI ìš”ì•½ ìƒì„± (ì•„ì§ ì—†ê±°ë‚˜ ì˜¤ë˜ëœ ê²½ìš°)
        # should_generate_ai_summary = (
        #     not wiki_place.ai_summation or
        #     not wiki_place.ai_summary_updated_at or
        #     (timezone.now() - wiki_place.ai_summary_updated_at).days > 30
        # )
        
        # if should_generate_ai_summary:
        # try:
        
            # ai_summary, ai_metadata = openai.generate_ai_summary(
            #     place_name=shop_name,
            #     reviews=review_texts,
            #     basic_info=wiki_place.basic_information
            # )
            
            # # AI ìš”ì•½ ì •ë³´ ì—…ë°ì´íŠ¸
            # wiki_place.ai_summation = ai_summary
            # wiki_place.ai_summation_info = ai_metadata
            # wiki_place.ai_summary_updated_at = timezone.now()
            # wiki_place.save(update_fields=[
            #     'ai_summation', 
            #     'ai_summation_info', 
            #     'ai_summary_updated_at'
            # ])
            
            # except Exception as e:
            #     logger.warning(f"AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        
        

        
        
        )
    
    @extend_schema(
        tags=["ìœ„í‚¤ ê¸°íƒ€"],
        parameters=[
            OpenApiParameter(name="limit", description="ë°˜í™˜í•  í‚¤ì›Œë“œ ê°œìˆ˜", required=False, type=int),
        ],
        responses={200: PopularKeywordSerializer(many=True)},
        description="ì¸ê¸° ê²€ìƒ‰ì–´ ëª©ë¡ ì¡°íšŒ"
    )
    @action(detail=False, methods=["GET"])
    def popular_keywords(self, request):
        """ì¸ê¸° ê²€ìƒ‰ì–´ ëª©ë¡ ë°˜í™˜"""
        limit = int(request.query_params.get('limit', 10))
        limit = min(max(limit, 1), 50)  # 1~50 ë²”ìœ„ë¡œ ì œí•œ
        
        try:
            keywords = get_popular_search_keywords(limit=limit)
            serializer = PopularKeywordSerializer(keywords, many=True)
            return Response(serializer.data, status=status.HTTP_200_OK)
        except Exception as e:
            logger.error(f"ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return Response(
                {'detail': 'ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )


        
        
        # try:
        #     # ê¸°ì¡´ Place ë˜ëŠ” WikiPlace ì°¾ê¸°
        #     place = None
        #     wiki_place = None
            
        #     # ì¢Œí‘œ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì¡´ ì¥ì†Œ ì°¾ê¸°
        #     place = Place.objects.filter(
        #         longitude__range=(longitude - 0.001, longitude + 0.001),
        #         latitude__range=(latitude - 0.001, latitude + 0.001)
        #     ).first()
            
        #     # WikiPlace ì°¾ê¸° ë˜ëŠ” ìƒì„±
        #     if place:
        #         wiki_place, created = WikiPlace.objects.get_or_create(
        #             place=place,
        #             defaults={
        #                 'shop_name': place_name,
        #                 'kakao_place_id': '',  # ê²€ìƒ‰ì—ì„œ ì˜¨ ê²½ìš° ë³„ë„ ì—…ë°ì´íŠ¸ í•„ìš”
        #             }
        #         )
        #     else:
        #         # ìƒˆ Place ìƒì„±
        #         with transaction.atomic():
        #             place = Place.objects.create(
        #                 name=place_name,
        #                 address=location_name,
        #                 dong='',  # ë³„ë„ API í˜¸ì¶œë¡œ ì±„ìš¸ ìˆ˜ ìˆìŒ
        #                 longitude=longitude,
        #                 latitude=latitude,
        #                 number='',
        #                 running_time=''
        #             )
                    
        #             wiki_place = WikiPlace.objects.create(
        #                 place=place,
        #                 shop_name=place_name
        #             )
            
        # # ê¸°ë³¸ ì •ë³´ êµ¬ì„± (ì—†ëŠ” ê²½ìš°)
        # if not wiki_place.basic_information:
        #     basic_info_parts = []
        #     if place.running_time:
        #         basic_info_parts.append(f"ìš´ì˜ì‹œê°„: {place.running_time}")
        #     if place.number:
        #         basic_info_parts.append(f"ì „í™”ë²ˆí˜¸: {place.number}")
        #     if place.address:
        #         basic_info_parts.append(f"ì£¼ì†Œ: {place.address}")
            
        #     wiki_place.basic_information = '\n'.join(basic_info_parts) or "ê¸°ë³¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        #     wiki_place.basic_information_info = {
        #         'generated_at': timezone.now().isoformat(),
        #         'source': 'internal_data'
        #     }
        #     wiki_place.save(update_fields=['basic_information', 'basic_information_info'])
        
        # # ë¦¬ë·° í†µê³„ ì—…ë°ì´íŠ¸
        # wiki_place.update_review_stats()
        
        # # ë¦¬ë·° ë°ì´í„° ì§ë ¬í™”
        # review_data = []
        # for review in reviews:
        #     review_data.append({
        #         'id': review.id,
        #         'content': review.review_content,
        #         'score': float(review.review_score),
        #         'created_at': review.created_at.isoformat(),
        #         'ai_review': review.ai_review,
        #     })

        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        # ai_summary = {
        #     'ai_summation': wiki_place.ai_summation,
        #     'ai_summation_info': wiki_place.ai_summation_info,
        #     'basic_information': wiki_place.basic_information,
        #     'basic_information_info': wiki_place.basic_information_info,
        # }
            
        
            
        #     serializer = WikiPlaceDetailSerializer(response_data)
        #     return Response(serializer.data, status=status.HTTP_200_OK)
            
        

    